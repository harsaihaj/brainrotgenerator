{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Khf6tvAH6mBm",
        "outputId": "102b7d93-4cf9-4397-ad2e-fb1e003b1dea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m174.3/174.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install yt-dlp moviepy gTTS praw --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2fUT4pk_J19",
        "outputId": "85167e71-85a8-4115-cf18-4a2f2a878066"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import praw\n",
        "import textwrap\n",
        "from gtts import gTTS\n",
        "from moviepy.editor import *\n",
        "\n",
        "# Folder structure\n",
        "os.makedirs(\"clips\", exist_ok=True)\n",
        "os.makedirs(\"tts\", exist_ok=True)\n",
        "os.makedirs(\"output\", exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vcN0zU_dJEA",
        "outputId": "d45da81e-2b37-406c-8b1a-81a7b9d47a1a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” Searching until enough valid posts are found...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Found post 1 (925 chars)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Found post 2 (944 chars)\n",
            "\n",
            "--- POST 1 (925 chars) ---\n",
            "Im (26m) raised my voice on her (25f) after she jokingly wanted to stick a magnet on pc my dad gave me.\n",
            "\n",
            "Soo i had traumas with magnet on pc because i saw effect of magnet on old pc at school before and she jokingly wanted to stick a magnet on my pc and i raised my voice on her because i dont think she knows what happens when u stick a magnet on a running pc and im scared it will break as i cherish the gaming computer so much its my only escape on sad life. I told her not to stick as it might damage pc but she kept going on with her joke soo i got scared and raised my voice so that she wouldnt stick it on the computer and shee got sad or mad?i applogized shortly after but Started crying on her home (she pmed) and told me how she got hurt and all i said sorrry and explained why i have to do it but i dont think she gets it.. now she would just seen me on msgs and told me she got fever isn't that a bit overreacting\n",
            "\n",
            "\n",
            "--- POST 2 (944 chars) ---\n",
            "My 23F husband 24M searched for his exes nudes online.\n",
            "\n",
            "I 23F caught my husband 24M was googling his exes name, her tattoo descriptions, trying to find nudes and videos of her. Weve been together for two years, he cheated on me with her very early on, she didn't know I existed. They dated for two years, about a year before we did, and he's been obsessed since. He told me he even put photos of her in a forum online, to try and get others to find her stuff for him!! Says he was just curious, bc I told her years ago that she'd done porn stuff. He wanted to \"prove me wrong\" that she wouldn't do that?!? But never told me. And on between these searches for her? Porn sites. We have two kids, no money, and I am not even working rn due to the kids and health stuff. Wtf do I even do and please tell me this isn't normal or okay, he's gaslighting me. \n",
            "\n",
            "TLDR; MY husband obsessively searched for his exes porn. The same ex he cheated on me with.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import praw\n",
        "\n",
        "reddit = praw.Reddit(\n",
        "    client_id=\"YOUR-CLIENT-ID\",\n",
        "    client_secret=\"YOUR-CLIENT-SECRET\",\n",
        "    user_agent=\"YOUR-REDDIT-USERNAME\"\n",
        ")\n",
        "\n",
        "def get_enough_posts(subreddit_name=\"relationship_advice\", limit=5, min_chars=925, max_chars=950):\n",
        "    posts = []\n",
        "    print(\"ğŸ”Searching until enough valid posts are found...\")\n",
        "\n",
        "    for post in reddit.subreddit(subreddit_name).hot(limit=None):  # unlimited scrolling\n",
        "        if post.stickied:\n",
        "            continue\n",
        "\n",
        "        title = post.title.strip()\n",
        "        body = post.selftext.strip()\n",
        "\n",
        "        if body.lower() in [\"[deleted]\", \"[removed]\"]:\n",
        "            continue\n",
        "\n",
        "        combined = f\"{title}\\n\\n{body}\" if body else title\n",
        "        length = len(combined)\n",
        "\n",
        "        if min_chars <= length <= max_chars:\n",
        "            posts.append(combined)\n",
        "            print(f\"âœ… Found post {len(posts)} ({length} chars)\")\n",
        "\n",
        "        if len(posts) >= limit:\n",
        "            break\n",
        "\n",
        "    return posts\n",
        "\n",
        "posts = get_enough_posts(limit=5, min_chars=925, max_chars=950)\n",
        "\n",
        "for i, p in enumerate(posts):\n",
        "    print(f\"\\n--- POST {i+1} ({len(p)} chars) ---\\n{p}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DN7TJdxqgAs",
        "outputId": "7e134b1e-997b-4ad0-d4ce-01d6e547179d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"Im (26m) raised my voice on her (25f) after she jokingly wanted to stick a magnet on pc my dad gave me.\\n\\nSoo i had traumas with magnet on pc because i saw effect of magnet on old pc at school before and she jokingly wanted to stick a magnet on my pc and i raised my voice on her because i dont think she knows what happens when u stick a magnet on a running pc and im scared it will break as i cherish the gaming computer so much its my only escape on sad life. I told her not to stick as it might damage pc but she kept going on with her joke soo i got scared and raised my voice so that she wouldnt stick it on the computer and shee got sad or mad?i applogized shortly after but Started crying on her home (she pmed) and told me how she got hurt and all i said sorrry and explained why i have to do it but i dont think she gets it.. now she would just seen me on msgs and told me she got fever isn't that a bit overreacting\",\n",
              " 'My 23F husband 24M searched for his exes nudes online.\\n\\nI 23F caught my husband 24M was googling his exes name, her tattoo descriptions, trying to find nudes and videos of her. Weve been together for two years, he cheated on me with her very early on, she didn\\'t know I existed. They dated for two years, about a year before we did, and he\\'s been obsessed since. He told me he even put photos of her in a forum online, to try and get others to find her stuff for him!! Says he was just curious, bc I told her years ago that she\\'d done porn stuff. He wanted to \"prove me wrong\" that she wouldn\\'t do that?!? But never told me. And on between these searches for her? Porn sites. We have two kids, no money, and I am not even working rn due to the kids and health stuff. Wtf do I even do and please tell me this isn\\'t normal or okay, he\\'s gaslighting me. \\n\\nTLDR; MY husband obsessively searched for his exes porn. The same ex he cheated on me with.']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "posts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D78uIFZS_TWq",
        "outputId": "0f47f5dd-3591-4925-da8f-d71879fe0b0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "def download_video(url, output=\"restock_full.mp4\"):\n",
        "    import yt_dlp\n",
        "    ydl_opts = {'outtmpl': output, 'format': 'mp4', 'quiet': True}\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        ydl.download([url])\n",
        "    return output\n",
        "\n",
        "YT_URL = \"https://www.youtube.com/watch?v=-LZceM7L8AE\"\n",
        "video_path = download_video(YT_URL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpjEy5Cv_Wgt",
        "outputId": "2d567e32-ec54-412c-f4e4-1f7f69216d13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¬ Saved silent clip: clips/clip_01.mp4\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def cut_video(path, length=90, folder=\"clips\", max_clips=1):\n",
        "    video = VideoFileClip(path).without_audio()\n",
        "    duration = int(video.duration)\n",
        "    count = 0\n",
        "    for start in range(0, duration, length):\n",
        "        if count >= max_clips:\n",
        "            break\n",
        "        end = min(start + length, duration)\n",
        "        clip = video.subclip(start, end)\n",
        "        out_path = os.path.join(folder, f\"clip_{count+1:02d}.mp4\")\n",
        "        clip.write_videofile(out_path, fps=24, audio=False, logger=None)\n",
        "        print(f\"ğŸ¬ Saved silent clip: {out_path}\")\n",
        "        count += 1\n",
        "    video.close()\n",
        "    return count\n",
        "\n",
        "cut_video(\"restock_full.mp4\", max_clips=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcLmXh4N_a_G",
        "outputId": "047ad36a-078d-4f01-eba9-9db3e82a6dac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¤ Generating TTS for Post 1\n",
            "âœ… Saved: tts/voice_00.mp3\n",
            "ğŸ¤ Generating TTS for Post 2\n",
            "âœ… Saved: tts/voice_01.mp3\n"
          ]
        }
      ],
      "source": [
        "from gtts import gTTS\n",
        "import os\n",
        "\n",
        "os.makedirs(\"tts\", exist_ok=True)\n",
        "\n",
        "def text_to_speech(text, idx):\n",
        "    tts = gTTS(text)\n",
        "    out = f\"tts/voice_{idx:02d}.mp3\"\n",
        "    tts.save(out)\n",
        "    return out\n",
        "\n",
        "for idx, post in enumerate(posts):\n",
        "    print(f\"ğŸ¤ Generating TTS for Post {idx+1}\")\n",
        "    path = text_to_speech(post, idx)\n",
        "    print(f\"âœ… Saved: {path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSpvqAT2F2Y8"
      },
      "outputs": [],
      "source": [
        "from moviepy.editor import VideoFileClip, AudioFileClip\n",
        "\n",
        "def overlay_audio(video_path, audio_path, output_path):\n",
        "    video = VideoFileClip(video_path).without_audio()\n",
        "    audio = AudioFileClip(audio_path)\n",
        "    final = video.set_audio(audio)\n",
        "    final.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\")\n",
        "    print(f\"âœ… Merged video saved: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx6kbM14F3k8",
        "outputId": "2f3e908c-11c6-4b7d-f322-567b8c239192"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Building video output/merged_clip_01.mp4.\n",
            "MoviePy - Writing audio in merged_clip_01TEMP_MPY_wvf_snd.mp4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Done.\n",
            "Moviepy - Writing video output/merged_clip_01.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready output/merged_clip_01.mp4\n",
            "âœ… Merged video saved: output/merged_clip_01.mp4\n"
          ]
        }
      ],
      "source": [
        "overlay_audio(\"clips/clip_01.mp4\", \"tts/voice_01.mp3\", \"output/merged_clip_01.mp4\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtMLVBkoGAze",
        "outputId": "fc007620-6c3f-4573-aeff-2c079006cfc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-ulmk726l\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-ulmk726l\n",
            "  Resolved https://github.com/openai/whisper.git to commit dd985ac4b90cafeef8712f2998d62c59c3e62d22\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (10.7.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (0.9.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.11/dist-packages (from openai-whisper==20240930) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper==20240930)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803707 sha256=c40752ff2d2fad521129de9cf1df707b08324386525bee37dd6574967347de15\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-owk19jeb/wheels/1f/1d/98/9583695e6695a6ac0ad42d87511097dce5ba486647dbfecb0e\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20240930\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqEHbnOOG1W5"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "\n",
        "def transcribe_to_srt(video_path, srt_path):\n",
        "    model = whisper.load_model(\"base\")  # or \"small\" for faster results\n",
        "    result = model.transcribe(video_path, fp16=False)\n",
        "\n",
        "    def format_timestamp(seconds):\n",
        "        h = int(seconds // 3600)\n",
        "        m = int((seconds % 3600) // 60)\n",
        "        s = int(seconds % 60)\n",
        "        ms = int((seconds - int(seconds)) * 1000)\n",
        "        return f\"{h:02}:{m:02}:{s:02},{ms:03}\"\n",
        "\n",
        "    with open(srt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for i, segment in enumerate(result[\"segments\"]):\n",
        "            start = format_timestamp(segment[\"start\"])\n",
        "            end = format_timestamp(segment[\"end\"])\n",
        "            text = segment[\"text\"].strip()\n",
        "\n",
        "            f.write(f\"{i+1}\\n{start} --> {end}\\n{text}\\n\\n\")\n",
        "\n",
        "    print(f\"âœ… SRT saved to: {srt_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYnPjEPwG4au"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "def burn_subtitles(video_path, srt_path, output_path):\n",
        "    # WrapStyle=0 (smart wrapping), Alignment=2 (bottom center), FontSize=20, MarginV=60 (vertical margin)\n",
        "    style = (\n",
        "        \"force_style='FontSize=20,\"\n",
        "        \"Alignment=2,\"\n",
        "        \"MarginV=60,\"\n",
        "        \"WrapStyle=0'\"\n",
        "    )\n",
        "\n",
        "    cmd = [\n",
        "        \"ffmpeg\", \"-y\", \"-i\", video_path,\n",
        "        \"-vf\", f\"subtitles={srt_path}:{style}\",\n",
        "        \"-c:a\", \"copy\", output_path\n",
        "    ]\n",
        "    subprocess.run(cmd)\n",
        "    print(f\"âœ… Final video with burned subtitles saved to: {output_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkE7yja2G8e9",
        "outputId": "81cc1ae1-29c4-489e-ced0-c4444ad77c89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… SRT saved to: output/subtitles_01.srt\n",
            "âœ… Final video with burned subtitles saved to: output/final_with_subs_01.mp4\n"
          ]
        }
      ],
      "source": [
        "merged_video = \"output/merged_clip_01.mp4\"\n",
        "srt_file = \"output/subtitles_01.srt\"\n",
        "final_video = \"output/final_with_subs_01.mp4\"\n",
        "\n",
        "# Step 1: Transcribe and create subtitles\n",
        "transcribe_to_srt(merged_video, srt_file)\n",
        "\n",
        "# Step 2: Burn subtitles into the video\n",
        "burn_subtitles(merged_video, srt_file, final_video)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCzWn6lUM3Pw",
        "outputId": "90c55970-9705-4457-8972-dce69865ad4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Building video output/final_fast_60s.mp4.\n",
            "MoviePy - Writing audio in final_fast_60sTEMP_MPY_wvf_snd.mp4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Done.\n",
            "Moviepy - Writing video output/final_fast_60s.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready output/final_fast_60s.mp4\n",
            "ğŸš€ Final video saved: output/final_fast_60s.mp4\n"
          ]
        }
      ],
      "source": [
        "from moviepy.editor import VideoFileClip, vfx\n",
        "\n",
        "def final_clip_90s_then_speedup(input_path, output_path):\n",
        "    video = VideoFileClip(input_path).subclip(0, 90)\n",
        "\n",
        "    fast_video = video.fx(vfx.speedx, 1.5)\n",
        "\n",
        "    fast_video = fast_video.set_duration(60)\n",
        "\n",
        "    fast_video.write_videofile(output_path, fps=24, codec='libx264', audio_codec='aac')\n",
        "    print(f\"ğŸš€ Final video saved: {output_path}\")\n",
        "\n",
        "final_clip_90s_then_speedup(\"output/final_with_subs_01.mp4\", \"output/final_fast_60s.mp4\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSFepKW7Kpx2",
        "outputId": "0ac7252c-f0c0-425f-b27b-fb4afe3a249c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Writing audio in temp_audio.wav\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Done.\n",
            "Moviepy - Building video HAHA_output_pitch_down.mp4.\n",
            "MoviePy - Writing audio in HAHA_output_pitch_downTEMP_MPY_wvf_snd.mp4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MoviePy - Done.\n",
            "Moviepy - Writing video HAHA_output_pitch_down.mp4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready HAHA_output_pitch_down.mp4\n"
          ]
        }
      ],
      "source": [
        "import moviepy.editor as mp\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import os\n",
        "\n",
        "def pitch_down_video(input_path, output_path, semitones=-4):\n",
        "    video = mp.VideoFileClip(input_path)\n",
        "    audio_path = \"temp_audio.wav\"\n",
        "    video.audio.write_audiofile(audio_path)\n",
        "\n",
        "    y, sr = librosa.load(audio_path, sr=None)\n",
        "    y_shifted = librosa.effects.pitch_shift(y=y, sr=sr, n_steps=semitones)\n",
        "    shifted_audio_path = \"temp_shifted.wav\"\n",
        "    sf.write(shifted_audio_path, y_shifted, sr)\n",
        "\n",
        "    new_audio = mp.AudioFileClip(shifted_audio_path)\n",
        "    final_video = video.set_audio(new_audio)\n",
        "    final_video.write_videofile(output_path, codec='libx264', audio_codec='aac')\n",
        "\n",
        "    os.remove(audio_path)\n",
        "    os.remove(shifted_audio_path)\n",
        "\n",
        "pitch_down_video(\"/content/output/final_fast_60s.mp4\", \"HAHA_output_pitch_down.mp4\", semitones=-3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnjLzh65QCD2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
